%\def\year{2020}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
 \pdfinfo{/Title gm_hmm
} %Leave this	
\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\newcommand{\citep}{\cite}
\newcommand{\citealp}[1]{\citeauthor{#1} \citeyear{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usetikzlibrary{calc,shapes,positioning}
\usetikzlibrary{arrows}
\newcommand{\midarrow}{\tikz \draw[-triangle 90] (0,0) -- +(.1,0);}
% Be sure to use PDxF Latex
\pdfoutput=1

\usepackage[latin1]{inputenc}




\usepackage{caption}
\usepackage{bm}
\newcommand{\ubar}[1]{\mkern2mu\underline{\mkern-2mu #1\mkern-2mu}\mkern2mu}
% \allowdisplaybreaks
\usepackage{mystyle}
\newcommand{\ubm}[1]{\ubar{\bm{#1}}}
\newcommand{\ubmr}[2]{\ubar{\bm{#1}}^{(#2)}}


% \newcommand{\bmtr}[3]{\bm{#1}^{(#3)}_{#2}}

\newcommand{\bmtr}[3]{\bm{#1}^{(#3)}_{#2}}
\newcommand{\smtr}[3]{{#1}^{(#3)}_{#2}}


\usepackage{amsmath,graphicx}
% format A4
% \usepackage{vmargin}
% \setpapersize{A4} 

\RequirePackage{algorithm}
\RequirePackage{algorithmic}

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}


\graphicspath{{./images/}}


% \hypersetup{  
%   bookmarks=true,
%   backref=true,
%   pagebackref=false,
%   colorlinks=true,
%   linkcolor=blue,
%   citecolor=red,
%   urlcolor=blue,
%   pdftitle={Generative Model HMM},
%   pdfauthor={Dong Liu},
%   pdfsubject={}
% }


\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai20.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\setlength\titlebox{2.5in} % If your paper contains an overfull \vbox too high warning at the beginning of the document, use this
% command to correct it. You may not alter the value below 2.5 in

\title{Powering Hidden Markov Model by Generative Models}
\author{
  Dong Liu, Antoine Honoré, Saikat Chatterjee, and Lars~K. Rasmussen\\
  KTH Royal Institute of Technology, Stockholm, Sweden \\
  E-mail: \{doli, honore, sach, lkra\}@kth.se}


% \name{
% Dong Liu,
% Minh Thành Vu,
% Saikat Chatterjee,
% and Lars~K. Rasmussen
% }

%   \address{
%   KTH Royal Institute of Technology, Stockholm, Sweden \\
%   E-mail: \{doli, mtvu, sach, lkra\}@kth.se}

\begin{document}

\maketitle
\begin{abstract}
to be written...
\end{abstract}

\section{introduction}

\section{Preliminaries}
\begin{figure}[!h]
  \centering
  \begin{tikzpicture}
    \tikzstyle{enode} = [thick, draw=black, ellipse, inner sep = 1pt,  align=center]
    \tikzstyle{nnode} = [thick, rectangle, rounded corners = 2pt,minimum size = 0.8cm,draw,inner sep = 2pt]
    \node[enode] (g1) at (-0.5,1.8) {$p(\bm{x}| s=1; \bm{\Phi}_{1})$};
    \node[enode] (g2) at (-0.5,0.5) {$p(\bm{x}| s=2; \bm{\Phi}_{2})$};
    \node[enode] (gs) at (-0.5, -1.8) {$p(\bm{x}| s=|\Ss|; \bm{\Phi}_{|\Ss|})$};
    \node[enode] (x) at (4.5,1.5){$\ubm{x}\sim p(\ubm{x};\bm{H})$};

    \draw[dotted,line width=2pt] (0,-0.3) -- (0,-1.2);
    \filldraw[->] (1.9, 0.5)circle (2pt) --  (x) ;
    \draw[->] (g1) -- (1.8, 1.8);
    \draw[->] (g2) -- (1.8, 0.5);
    \draw[->] (gs) -- (1.8, -1.8);

    \begin{scope}[xshift=0.5cm, thick, every node/.style={sloped,allow upside down}]
      \node[nnode] (m) at (3.5,-2) {Memory};
      \node[nnode] (a) at (3.5,-0.5) {$\bm{A}$};

      \draw (2.1,0.9)-- (2.2, 0.);
      \draw (2.2,0.)-- node {\midarrow} (2.2,-2);
      \draw (2.2,-2)-- (m);
      \draw (m)-- (5, -2);
      \draw (5, -2)-- node {\midarrow} (5 ,-0.5);
      \draw (5, -0.5) -- (a);
      \draw (a)-- node {\midarrow} (2.2, -0.5);
      \node at (4.8, -1) {$s_{t}$};
      \node at (2.56, -0.25) {$s_{t+1}$};
    \end{scope}
  \end{tikzpicture}
  \caption{HMM model illustration.}\label{fig:hmm}
  \vspace{0.1cm}
\end{figure}

Our framework is an Hidden Markov Model (HMM). An HMM model $\bm{H}$ defined in a hypothesis space $\Hh$, i.e. $\bm{H} \in \Hh$, is capable to model time-span signal $\ubar{\bm{x}} = \left[ \bm{x}_1, \cdots, \bm{x}_T\right]^{\intercal}$, where $\bm{x}_t\in \RR^{N}$ is the signal at time $t$, $[\cdot]^{\intercal}$ means transpose, and $T$ denote the time length. Using HMM for signal representation or generating is illustrated in \ref{fig:hmm}. The assumption is that different instant signal of $\ubar{\bm{x}}$ is generated by different source, while the source selection is done by a discrete hidden markov process. We define the hypothesis set of HMM as $\Hh := \{\bm{H} | \{\Ss, \bm{q}, \bm{A}, p(\bm{x}|{s}; \bm{\Phi}_{s})\}$, where:
\begin{itemize}
\item $\Ss$ is the set of states of HMM $\bm{H}$;
\item $\bm{q} = \left[ q_1, q_2, \cdots, q_{|\Ss|}\right]^\intercal$ initial distribution of HMM $\bm{H}$ with $|\Ss|$ is cardinality of $\Ss$, $q_k = p(s=k)$ for random state variable $s$. In the following of this paper, we use $s_t$ to denote the state $s$ at time $t$.
\item $\bm{A}$ is the transition matrix for the HMM $\bm{H}$ of size $|\Ss| \times |\Ss|$, $\bm{A}_{i,j} = p(s_{t+1}=j|s_{t}=i)$.
\item For given hidden state $s$, the observable signal density is $p({\bm{x}}|{s};\bm{\Phi}_{s})$, where $\bm{\Phi}_{s}$ is the parameter set that defines this conditional probabilistic model.
\end{itemize}

In the framework of HMM, at each time instance, signal $\bm{x}_t$ is generated by density $p(\bm{x}_t| s_t; \bm{\Phi}_{s_t})$, and $s_t$ is decided by the hidden markov process. This process gives us the probabilistic model $p(\ubm{x};\bm{H})$
\section{Generator-mixed HMM}

\begin{figure}[!th]
  \centering
  \begin{tikzpicture}
    \tikzstyle{enode} = [thick, draw=black, ellipse, inner sep = 2pt,  align=center]
    \tikzstyle{nnode} = [thick, rectangle, rounded corners = 2pt,minimum size = 0.8cm,draw,inner sep = 2pt]
    \node[enode] (z1) at (-1.2,1.8) {$\bm{z}\sim p_{s,1}(\bm{z})$};
    \node[nnode] (g1) at (1,1.8) {$\bm{g}_{s,1}$};
    \node[enode] (z2) at (-1.2,0.5){$\bm{z}\sim p_{s,2}(\bm{z})$};
    \node[nnode] (g2) at (1,0.5) {$\bm{g}_{s,2}$};
    \node[enode] (zK) at (-1.2,-1.8) {$\bm{z}\sim p_{s,K}(\bm{z})$};
    \node[nnode] (gs) at (1, -1.8) {$\bm{g}_{s,K}$};
    \node[enode] (x) at (4.5,0){$\bm{x}\sim p(\bm{x}| s; \bm{\Phi}_{s})$};

    \draw[dotted,line width=2pt] (0,-0.3) -- (0,-1.2);
    \filldraw[->] (1.9, 0.5)circle (2pt) --  node[above=0.2]{${\kappa}\sim \bm{\pi}_{s}$} (x)  ;
    \draw[->] (z1) -- (g1);
    \draw[->] (g1) -- (1.8, 1.8);

    \draw[->] (z2) -- (g2);
    \draw[->] (g2) -- (1.8, 0.5);

    \draw[->] (zK) -- (gs);
    \draw[->] (gs) -- (1.8, -1.8);
  \end{tikzpicture}
  \caption{Source $s$ powered by generator mixed generative model (GenHMM).}
  \vspace{0.1cm}
\end{figure}

In this section, we introduce a neural network based generator-mixed HMM model. We term it as GenHMM. GenHMM has a hidden markov process as typical HMM, while the probabilistic model of GenHMM for each hidden state is induced by mixture of neural network based generators. For $s\in \Ss$, we define the state's probabilistic model in GenHMM as
\begin{equation}
  p(\bm{x}| s; \bm{\Phi}_{s}) = \sum_{\kappa=1}^{K}\pi_{s, \kappa} p(\bm{x}| s, \kappa; \bm{\Phi}_{s, \kappa})
\end{equation}
where $\kappa$ is a random variable following a categorical distribution, and $\pi_{s, \kappa} = p(\kappa | \bm{x}, s; \bm{\Phi}_{s, \kappa})$. Naturally $\sum_{\kappa = 1}^{K} \pi_{s, \kappa}= 1$. Denote $\bm{\pi}_{s} = [\pi_{s,1}, \pi_{s,2}, \cdots, \pi_{s,K}]$,
$\bm{\Phi}_s = \left\{ \pi_{s, \kappa}, \bm{\Phi}_{s, \kappa}| \kappa = 1, 2, \cdots, K \right\}$, 



\subsubsection{Generator Mixed HMM (GenHMM)}\label{subsubsec:GenHMM}
In this subsection, we specify the GenHMM model and give the learning algorithm of this model.
Base on the generative model assumption, for each state $s$, the probabilistic generative model is

\subsubsection{Flow structure}

\section{Learning Algorithm of GenHMM}
\subsection{Problem Statement}
Given a empirical distribution $\hat{p}(\ubm{x}) = \frac{1}{R}\sum_{r=1}^{R} \delta_{\ubmr{x}{r}}(\ubm{x})$. We want to find a probabilistic model such that:
\begin{equation}
  \min KL(\hat{p}(\ubm{x})\| p(\ubm{x}))
\end{equation}
where $KL(\cdot\|\cdot)$ denotes the Kullback-Leibler divergence.

When we use HMM to model the empirical distribution and approach the unknown true distribution, the problem boils down to:
\begin{equation}
  \uargmax{\bm{H} \in \Hh} p(\ubm{X}; \bm{H})
\end{equation}
where $\ubm{X} = \left[ \ubmr{x}{1}, \ubmr{x}{2}, \cdots, \ubmr{x}{R} \right]$ 

The problem can be reformulated as
\begin{equation}\label{eq:ml-of-hmm}
  \uargmax{\bm{H} \in \Hh} \sum_{r=1}^{R}\log\,p(\ubmr{x}{r}; \bm{H})
\end{equation}
for independent identical distributed assumption of $\ubm{x}$.

\subsection{Proposal}




Since model $\bm{H}$ contains hidden sequential variable $\ubm{s}$, we can not directly solve the maximum likelihood problem in \ref{eq:ml-of-hmm}. We use expectation maximization (EM) to address the hidden variable problem by
\begin{itemize}
\item E-step: % the posterior probability of $\ubm{s}$:
  % \begin{equation}
  %   p(\ubm{s}|\ubm{x})
  % \end{equation}
  The ``expected likelihood'' function:
  \begin{equation}\label{eq:em-q-funciton}
    \Qq(\bm{H}; \bm{H}^{\mathrm{old}}) = \ \sum_{r=1}^{R}E_{p(\ubmr{s}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})}\left[\log\,p(\ubmr{x}{r}, \ubmr{s}{r}; \bm{H}) \right]
  \end{equation}
\item M-step: the optimization step:
  \begin{equation}\label{eq:em-m-opt}
    \umax{\bm{H}} \Qq(\bm{H}; \bm{H}^{\mathrm{old}})
  \end{equation}
\end{itemize}


The \eqref{eq:em-m-opt} can be reformulated as:
\begin{equation}\label{eq:m-step-subs}
  \umax{\bm{H}} \Qq(\bm{H}; \bm{H}^{\mathrm{old}}) = \umax{\bm{q}}\Qq(\bm{q}; \bm{H}^{\mathrm{old}}) + \umax{{A}}\Qq({A}; \bm{H}^{\mathrm{old}}) + \umax{\bm{\Phi}}\Qq(\bm{\Phi}; \bm{H}^{\mathrm{old}})
\end{equation}
where
\begin{align}
  \Qq(\bm{q}; \bm{H}^{\mathrm{old}}) &= \sum_{r=1}^{R} \EE_{p(\ubmr{s}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})} \left[\log\,p(\smtr{s}{1}{r}; \bm{q})\right] \label{eq:init-distribution-update}\\
  \Qq(\bm{A}; \bm{H}^{\mathrm{old}}) &= \sum_{r=1}^{R} \EE_{p(\ubmr{s}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})} \left[\log\,\sum_{t=1}^{T^{(r)}-1}p(\smtr{s}{t+1}{r}|\smtr{s}{t}{r}; {A})\right] \label{eq:transition-update}\\
  \Qq(\bm{\Phi}; \bm{H}^{\mathrm{old}}) &= \sum_{r=1}^{R} \EE_{p(\ubmr{s}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})} \left[\log\,p(\ubmr{x}{r} | \ubmr{s}{r}; \bm{\Phi})\right]\label{eq:generative-model-update}
\end{align}

We can see that the solution of $H$ depends on the posterior probability $p(\ubm{s}| \ubm{x}; \bm{H})$. Though the evaluation of posterior according to Bayesian theorem is simple, the computation complexity of $p(\ubm{s}| \ubm{x}; \bm{H})$ grows exponentially with the length of $\ubm{s}$. Therefore, we would employ Forward/Backward algorithm \cite{} to do the posterior computation efficiently. The marginal $p(s_t| \ubm{x}; \bm{H})$ is also efficiently computed as the joint posterior. 

We summarize the optimization algorithm as:

\begin{algorithm}[H]
  \caption{Meta algorithm for HMM powered by Generative Models (GMHMM)}
  \begin{algorithmic}[1]
    \STATE {\bfseries Input:}{
      Building $\bm{H}^{\mathrm{old}}, \bm{H} \in \Hh$ gives: \\
      $\bm{H}^{\mathrm{old}} = \{\Ss, \bm{q}^{\mathrm{old}}, A^{\mathrm{old}}, p(\bm{x}|s; \bm{\Phi}^{\mathrm{old}})\}$, $\bm{H} = \{\Ss, \bm{q}, A, p(\bm{x}|s; \bm{\Phi})\}$
    }, \\
    \STATE Initialize $\bm{H}$
    \STATE $\bm{H}^{\mathrm{old}} \gets \bm{H}$
    \FOR { $\bm{H}$ not converge}
    \STATE Sample a batch of data $\left\{ \ubmr{x}{r} \right\}_{r=1}^{R_b}$ from the dataset $\hat{p}(\ubm{x})$
    \STATE Compute $p(\smtr{s}{t}{r} | \ubmr{x}{r}; \bm{H}^{\mathrm{old}})$, $p(\smtr{s}{t}{r}, \smtr{s}{t+1}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})$ by forward/backward algorithm;
    \STATE $\bm{q} \gets \uargmin{\bm{q}}\, \Qq(\bm{q}; \bm{H}^{\mathrm{old}})$ by \eqref{eq:update-initial-state-prob};
    \STATE $\bm{A} \gets \uargmin{\bm{A}}\Qq(\bm{A}; \bm{H}^{\mathrm{old}})$ by \eqref{eq:update-transition-prob};
    \STATE $\bm{\Phi} \gets \uargmin{\bm{\Phi}}\Qq(\bm{\Phi}; \bm{H}^{\mathrm{old}})$ by calling Algorithm~\ref{algo:opt-Phi-GenHMM} or \ref{algo:opt-Phi-LatHMM};
    \STATE $\bm{H}^{\mathrm{old}} \gets \bm{H}$

    \ENDFOR
  \end{algorithmic}
\end{algorithm}


\subsection{Initial Probability Update}
\eqref{eq:init-distribution-update} can be written as:
\begin{align}
  \Qq(\bm{q}; \bm{H}^{\mathrm{old}}) &= \sum_{r=1}^{R}\sum_{\ubmr{s}{r}} {p(\ubmr{s}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})} \log\,p(\smtr{s}{1}{r}; \bm{q}) \nonumber \\
                                     & = \sum_{r=1}^{R}\sum_{\smtr{s}{1}{r}=1}^{|\Ss|}\sum_{\smtr{s}{2}{r}=1}^{|\Ss|}\cdots \sum_{\smtr{s}{T^{r}}{r}}^{{|\Ss|}} {p(\smtr{s}{1}{r}, \smtr{s}{2}{r}, \cdots, \smtr{s}{T^{r}}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})} \log\,p(\smtr{s}{1}{r}; \bm{q}) \\
                                     & = \sum_{r=1}^{R}\sum_{\smtr{s}{1}{r}=1}^{|\Ss|}{p(\smtr{s}{1}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})} \log\,p(\smtr{s}{1}{r}; \bm{q}) 
\end{align}

Since $p(\smtr{s}{1}{r}; \bm{H})$ is the probability of initial state of HMM $\bm{H}$ for $r$-th sequential, actually $q_i = p(\smtr{s}{1}{r} =i;\bm{H} ) $ for $i= 1, 2, \cdots, |\Ss|$. Solution to problem:
\begin{align}
  \bm{q}^{\mathrm{new}} &= \uargmax{\bm{q}} \Qq(\bm{q}; \bm{H}^{\mathrm{old}}), \nonumber \\
                        &\mathrm{s.t.} \sum_{i=1}^{ |\Ss| }q_i = 1 \nonumber\\
                        &q_i \geq 0, \forall s.
\end{align}
is
\begin{equation}\label{eq:update-initial-state-prob}
  q_i = \frac{1}{R} \sum_{r=1}^{R} p(\smtr{s}{1}{r}=i | \ubmr{x}{r}; \bm{H}^{\mathrm{old}}), \forall\; i = 1, 2, \cdots, |\Ss|.
\end{equation}

\subsection{Transition Probability Update}
\eqref{eq:transition-update} can be written as
\begin{align}
  \Qq(\bm{A}; \bm{H}^{\mathrm{old}})
  &= \sum_{r=1}^{R} \EE_{p(\ubmr{s}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})} \left[\log\,\sum_{t=1}^{T^{(r)}-1}p(\smtr{s}{t+1}{r}|\smtr{s}{t}{r}; {A})\right] \nonumber\\
  &= \sum_{r=1}^{R} \sum_{\ubmr{s}{r}}{p(\ubmr{s}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})} \log\,\sum_{t=1}^{T^{(r)}-1}p(\smtr{s}{t+1}{r}|\smtr{s}{t}{r}; {A}) \nonumber \\
  &= \sum_{r=1}^{R} \sum_{t=1}^{T^{(r)}-1} \sum_{\smtr{s}{t}{r}=1}^{|\Ss|}\sum_{\smtr{s}{t+1}{r}=1}^{|\Ss|}{p(\smtr{s}{t}{r}, \smtr{s}{t+1}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})} \log\,p(\smtr{s}{t+1}{r}|\smtr{s}{t}{r}; {A})
\end{align}

Since $\bm{A}_{i, j}  = p(\smtr{s}{t+1}{r}=j|\smtr{s}{t}{r}=i; {A})$ where $A_{i, j}$ is the element of transition matrix $A$, the solution to problem:
\begin{align}\label{eq:update-transition-prob}
  \bm{A}^{\mathrm{new}} = &\uargmax{\bm{A}} \Qq(\bm{A}; \bm{H}^{\mathrm{old}}), \nonumber \\
  \mathrm{s.t.} &\hspace{0.2cm} \bm{A} \cdot \bm{1} = \bm{1} \nonumber \\
                          &  \bm{A}^{\intercal} \cdot \bm{1} = \bm{1} \nonumber \\
                          & \bm{A}_{i,j} \geq 0.
\end{align}
is
\begin{equation}
  \bm{A}_{i,j}^{\mathrm{new}} = \frac{\bar{\xi}_{i,j}}{\sum_{k = 1}^{|\Ss|} \bar{\xi}_{i,k}},
\end{equation}
where
\begin{equation}
  \bar{\xi}_{i,j} = \sum_{r= 1}^{R} \sum_{t= 1}^{T^{(r)}-1}{p(\smtr{s}{t}{r}=i, \smtr{s}{t+1}{r}=j| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})}
\end{equation}

\subsection{Generative Model Update}
\eqref{eq:generative-model-update} can be rewritten as
\begin{align}\label{eq:gm-update}
  \Qq(\bm{\Phi}; \bm{H}^{\mathrm{old}})
  &= \sum_{r=1}^{R}\sum_{\ubmr{s}{r}}{p(\ubmr{s}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})} \log\,p(\ubmr{x}{r} | \ubmr{s}{r}; \bm{\Phi}) \nonumber \\
  &= \sum_{r=1}^{R} \sum_{t=1}^{{T}^{(r)}-1} \sum_{\smtr{s}{t}{r}=1}^{|\Ss|}  p(\smtr{s}{t}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}}) \log\, p(\bmtr{x}{t}{r} | \smtr{s}{t}{r}; \bm{\Phi}).
\end{align}

Then the third subproblem of \eqref{eq:m-step-subs} becomes:
\begin{align}\label{eq:sub-gm}
  &\uargmax{\bm{\Phi}} \Qq(\bm{\Phi}; \bm{H}^{\mathrm{old}}), \nonumber \\
  \mathrm{s.t.} &\hspace{0.2cm} p(\bm{x} | {s}; \bm{\Phi}) \mathrm{~is~our~ general~model}
\end{align}

It could be seen from \eqref{eq:gm-update} that the key to update generate  model is to evaluate $p(\bm{x}|{s}; \bm{\Phi})$ for all $s \in \Ss$. In Forward/Backward algorithm, evaluation of $p(\bm{x}|{s}; \bm{\Phi})$ is also all what is needed to compute $p(s|\bm{x};\bm{\Phi})$. In the following two subsections, we will provide two neural network based generative models that fulfill this requirement and also have high capability for complex signal modeling.




% Let us denote the inverse of $\bm{g}_s$ as $\bm{f}_s = \bm{g}_s^{-1}$.
% For this case we assume the latent sources are Gaussian. We write the emission probability model as a $K$ generator mixed model:
% \begin{align}
    %     p(\bm{x} | s; \bm{\Phi}) &= \sum_{\kappa = 1}^{K} \pi_{s, \kappa} \Nn\left( \bm{f}_{s,\kappa}(\bm{x}); \bm{\mu}_{s,\kappa}, \bm{C}_{s,\kappa}  \right)\bigg|\det\,\left( \pd{\bm{f_{s,\kappa}(\bm{x})}}{\bm{x}} \right)\bigg| \nonumber \\
    %     \sum_{\kappa = 1}^{K} \pi_{s, \kappa}&= 1
                                                 %   \end{align}

The help function should be revised to deal with the new latent variable $\kappa$ into:
\begin{align}\label{eq:obj-q-gen-mix}
  \Qq(\bm{\Phi}; \bm{H}^{\mathrm{old}}) & = \sum_{r=1}^{R} \sum_{t=1}^{{T}^{(r)}-1} \sum_{\smtr{s}{t}{r}=1}^{|\Ss|} \sum_{\smtr{\kappa}{t}{r}=1}^{K}p(\smtr{s}{t}{r}, \smtr{\kappa}{t}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})  \log\,p(\smtr{\kappa}{t}{r}, \bmtr{x}{t}{r}| \smtr{s}{t}{r}; \bm{\Phi}_{s})\nonumber \\
  =& \sum_{r=1}^{R} \sum_{t=1}^{{T}^{(r)}-1} \sum_{\smtr{s}{t}{r}=1}^{|\Ss|}  \sum_{\smtr{\kappa}{t}{r}=1}^{K}p(\smtr{s}{t}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})p(\smtr{\kappa}{t}{r}|\smtr{s}{t}{r}, \ubmr{x}{r}; \bm{H}^{\mathrm{old}}) \bigg[\log\,\pi_{\smtr{s}{t}{r},\smtr{\kappa}{t}{r}} \nonumber \\
                                        & + \log\, p(\bmtr{x}{t}{r} | \smtr{s}{t}{r}, \smtr{\kappa}{t}{r}; \bm{\Phi}_s) \bigg]
\end{align}
                                          %                                           \Nn\left( \bm{f}_{\smtr{s}{t}{r},\smtr{\kappa}{t}{r}}(\bm{x}); \bm{\mu}_{\smtr{s}{t}{r},\smtr{\kappa}{t}{r}}, \bm{C}_{\smtr{s}{t}{r},\smtr{\kappa}{t}{r}} \right) \log\,\bigg| \det\, \left( \pd{\bm{f}_{\smtr{s}{t}{r},\smtr{\kappa}{t}{r}}(\bmtr{x}{t}{r})}{\bmtr{x}{t}{r}} \right) \bigg| \bigg].
                                          %   \end{align}
In \eqref{eq:obj-q-gen-mix}, $p(s_t| \ubm{x}, \bm{H}^{\mathrm{old}})$ is computed by forward/backward algorithm. The posterior of $\kappa$ is:
\begin{align}\label{eq:kappa-posterior}
  p(\kappa| s, \ubm{x}; \bm{H}^{\mathrm{old}})
  &=  \frac{p(\kappa, \ubm{x}| s; \bm{H}^{\mathrm{old}})}{p(\ubm{x}| s,\bm{H}^{\mathrm{old}})} \nonumber \\
  & = \frac{\pi_{s, \kappa} p(\ubm{x}| s, \kappa, \bm{H}^{\mathrm{old}})}{\sum_{\kappa=1}^{K} \pi_{s, \kappa} p(\ubm{x}| s, \kappa,\bm{H}^{\mathrm{old}})} \nonumber \\
  & = \frac{\pi_{s, \kappa} p(\bm{x}| s, \kappa, \bm{H}^{\mathrm{old}})}{\sum_{\kappa=1}^{K}  \pi_{s, \kappa} p(\bm{x}| s, \kappa,\bm{H}^{\mathrm{old}})} 
  % & = \frac{\pi_{s_t, \kappa_t} \Nn\left( \bm{f}_{{s}_{t},{\kappa}_{t}}(\bm{x}); \bm{\mu}_{{s}_{t},{\kappa}_{t}}, \bm{C}_{{s}_{t},{\kappa}_{t}} \right) \bigg| \det\, \left( \pd{\bm{f}_{{s}_{t},{\kappa}_{t}}(\bmtr{x}{t}{r})}{\bmtr{x}{t}{r}} \right) \bigg| }{\sum_{\kappa_t=1}^{K}\pi_{s_t, \kappa_t} \Nn\left( \bm{f}_{{s}_{t},{\kappa}_{t}}(\bm{x}); \bm{\mu}_{{s}_{t},{\kappa}_{t}}, \bm{C}_{{s}_{t},{\kappa}_{t}} \right) \bigg| \det\, \left( \pd{\bm{f}_{{s}_{t},{\kappa}_{t}}(\bmtr{x}{t}{r})}{\bmtr{x}{t}{r}} \right) \bigg|} \nonumber \\
\end{align}
where the last equation is due to the fact that only $\bm{x}_t$ among sequence $\ubm{x}$ depends on $s_t, \kappa_t$. 



    %     \begin{align}

    %   \end{align}

The latent prior for mixture of each source $s$ is obtained by solving the following problem:
\begin{align}\label{opm:pi}
  \pi_{s, \kappa} & = \uargmax{\pi_{s, \kappa}} \Qq(\bm{\Phi}; \bm{H}^{\mathrm{old}}) \\ \nonumber
                  & s.t. \, \sum_{\kappa=1}^{K} \pi_{s, \kappa}= 1, \forall s = 1, 2, \cdots, |\Ss|. 
\end{align}

To solve problem \eqref{opm:pi}, we formulate its Lagrange function as
\begin{equation}
  \Ll = \Qq(\bm{\Phi}; \bm{H}^{\mathrm{old}}) + \sum_{\lambda_s} \lambda_s\left( 1-  \sum_{\kappa=1}^{K}\pi_{s, \kappa}  \right).
\end{equation}
Solving $\pd{\Ll}{\pi_{s, \kappa}} = 0$ gives
\begin{equation}
  \pi_{s,\kappa} = \frac{1}{\lambda_s}\sum_{r=1}^{R} \sum_{t=1}^{{T}^{(r)}} p(\smtr{s}{t}{r}=s, \smtr{\kappa}{t}{r} =\kappa| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})
\end{equation}
With condition $\sum_{\kappa=1}^{K} \pi_{s, \kappa}= 1, \forall s = 1, 2, \cdots, |\Ss|$, we have
\begin{equation}
  \lambda_s = \sum_{\kappa=1}^{K}\sum_{r=1}^{R} \sum_{t=1}^{{T}^{(r)}} p(\smtr{s}{t}{r}=s, \smtr{\kappa}{t}{r} =\kappa | \ubmr{x}{r}; \bm{H}^{\mathrm{old}})
\end{equation}
Then the solution to \eqref{opm:pi} is
\begin{equation}\label{eq:mix-latent-parameter-solution}
  \pi_{s, \kappa} = \frac{\sum_{r=1}^{R} \sum_{t=1}^{{T}^{(r)}} p(\smtr{s}{t}{r} =s, \smtr{\kappa}{t}{r}=\kappa | \ubmr{x}{r}; \bm{H}^{\mathrm{old}}) }{\sum_{k =1}^{K}\sum_{r=1}^{R} \sum_{t=1}^{{T}^{(r)}-1} p(\smtr{s}{t}{r} =s, \smtr{\kappa}{t}{r}=k | \ubmr{x}{r}; \bm{H}^{\mathrm{old}}) }
\end{equation}
where $p(s, \kappa | \ubm{x}; \bm{H}^{\mathrm{old}}) = p(s| \ubm{x}; \bm{H}^{\mathrm{old}}) p(\kappa | s, \ubm{x}; \bm{H}^{\mathrm{old}})$. $p(s| \ubm{x}; \bm{H}^{\mathrm{old}})$ that can be computed by results of forward/backward and \eqref{eq:kappa-posterior}. The posterior for $\kappa$ can be obtained by
\begin{equation}
  p(\kappa | s, \ubm{x}; \bm{H}^{\mathrm{old}}) = \frac{p(\kappa, \ubm{x} | s; \bm{H}^{\mathrm{old}})}{\sum_{\kappa}{p(\kappa, \ubm{x} | s; \bm{H}^{\mathrm{old}})}} = \frac{\pi_{s, \kappa} p( \bm{x} | s; \bm{H}^{\mathrm{old}})}{\sum_{\kappa}{\pi_{s, \kappa} p(\bm{x} | s; \bm{H}^{\mathrm{old}})}}
  \end{equation}


In implementation, GenHMM uses the generator mixed emission model with $\kappa$-th component as:

\begin{align}
  &p(\bm{x}| s, \kappa; \bm{\Phi}_{s, \kappa}) \nonumber \\
  =&p_{s, \kappa}(\bm{z}) \bigg| \det \left( \pd{\bm{z}}{\bm{x}} \right) \bigg| \nonumber \\
  = &\Nn\left( \bm{f}_{{s},{\kappa}}(\bm{x}); \bm{\mu}_{{s},{\kappa}}, \bm{C}_{{s},{\kappa}} \right) \bigg| \det\, \left( \pd{\bm{f}_{{s},{\kappa}}(\bm{x})}{\bm{x}} \right) \bigg|
\end{align}
where $\bm{f}_{{s},{\kappa}} = \bm{g^{-1}_{{s},{\kappa}}}$ is defined by parameter set $\bm{\theta}_{s, \kappa}$, and $\bm{\Phi}_{s, \kappa} = \left\{\bm{\mu}_{{s},{\kappa}}, \bm{C}_{{s},{\kappa}} , \bm{\theta}_{s, \kappa} \right\}, \kappa = 1, 2, \cdots, K$. We can start by setting $\bm{\mu}_{{s},{\kappa}} = \bm{0}$ and $\bm{C}_{{s},{\kappa}} = diag(\bm{1})$.
\begin{algorithm}[H]
  \caption{M-step w.r.t. $\bm{\Phi}$ powered by GenMM}\label{algo:opt-Phi-GenHMM}
  \begin{algorithmic}[1]
    \STATE {\bfseries Input:}
    Latent mixture distribution: $\Nn\left(\bm{z}; \bm{0}, \mathrm{diag}(\bm{1})\right)$, $\forall \, s \in \Ss, \kappa = 1, 2, \cdots, K$;\\
    Empirical distribution $\hat{p}(\bm{x})$ of dataset; \\
    \STATE Set a total number of epochs $T$ of training as stop criterion. A learning rate $\eta$.
    \FOR {epoch $t < T$}
    \STATE Sample a batch of data $\left\{ \ubmr{x}{r} \right\}_{r=1}^{R_b}$ from dataset $P_d(\ubm{x})$
    
    \STATE Compute $p(\smtr{s}{t}{r}, \smtr{\kappa}{t}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}})$  by Forward/backward and \eqref{eq:kappa-posterior}.
    %     = \frac{\pi_k \Nn\left(f(X); \bm{\mu}_k^{t},\bm{\sigma}_k^{t}\right)}{\sum_{k=1}^K\; \pi_k\Nn\left(f(X); \bm{\mu}_k^{t},\bm{\sigma}_k^{t}\right) }, \forall \left\{ x^{(i)}\right\}_{i=1}^{n_b}$
    % \Comment{Calculate posterior \autoref{eq-posterior-gamma}}
    \STATE Compute loss ${\Qq}\left({\bm{\Phi}}, {\bm{H}}^{\mathrm{old}}\right)$ in \eqref{eq:obj-q-gen-mix}

    \STATE $\partial{\bm{\th}_s} \gets  \nabla_{\bm{\theta}} -
    \frac{1}{R_{b}}{\Qq}\left({\bm{\Phi}},{\bm{H}}^{\mathrm{old}}\right)$
    % \Comment{E-step}
    % \STATE $\partial{g_k} \gets \nabla_{\th_k}\widetilde{\Qq}\left(\left\{ \th_k
    %   \right\}_{k=1}^{K};\left\{ \th_k^t\right\}_{k=1}^{K}\right)$,
    % $\forall \th_k \in\left\{ \th_k\right\}_{k=1}^{K}$
    \STATE $\bm{\th}_s \gets \bm{\th}_s - \eta \cdot \partial{\bm{\theta}_s}$, $\forall s \in \Ss$
    \ENDFOR
    
    \STATE Update $\pi_{s, \kappa}, \forall, s\in \Ss, \kappa= 1, 2, \cdots, K$, according to \eqref{eq:mix-latent-parameter-solution}
        
    \STATE Assemble $\bm{\Phi} = \left\{  \bm{\Phi}_s| s \in \Ss\right\}$ for $\bm{\Phi}_s = \left\{ \bm{\theta}_{s, \kappa}, \bm{\mu}_{{s},{\kappa}}, \bm{C}_{{s},{\kappa}}| \kappa = 1, 2, \cdots, K \right\}$.
    
    
  \end{algorithmic}
\end{algorithm}

\subsubsection{Generator-tied HMM (GtHMM)}
\begin{figure}[!h]
  \centering
  \begin{tikzpicture}
    \tikzstyle{enode} = [thick, draw=blue, ellipse, inner sep = 1pt,  align=center]
                                           %                                            \tikzstyle{nnode} = [thick, rectangle, rounded corners = 1pt,minimum size = 1.2cm,draw,inner sep = 2pt]
    \tikzstyle{nnode} = [thick, rectangle, rounded corners = 2pt,minimum size = 0.8cm,draw,inner sep = 2pt]
    \node[enode] (z1) at (0,1.8) {$\bm{y}\sim p_1(\bm{y})$};
    \node[enode] (z2) at (0,0.5){$\bm{y}\sim p_{2}(\bm{y})$};
    \node[enode] (zK) at (0,-1.8) {$\bm{y}\sim p_{|\Ss|}(\bm{y})$}; 
    \node[enode] (x) at (5.8,1){$\ubm{x}\sim p(\ubm{x};\bm{H})$};
                                           %                                            \node at (3,-1){$p(\bm{z}) = \sum_k\pi_kp_k(\bm{z})$};
    \node[nnode] (g) at (3.5,1) {$\bm{g}$};

    \node[nnode] (m) at (3.5,-2) {Memory};
    \node[nnode] (a) at (3.5,-0.5) {${A}$};


    \draw[dotted,line width=2pt] (0,-0.3) -- (0,-1.2);
    \filldraw[->] (1.8, 0.5)circle (2pt) --  (g) ;
    \draw[->] (z1) -- (1.6, 1.8);
    \draw[->] (z2) -- (1.6, 0.5);
                                           %                                            \draw[->] (1,-0.5) -- (1.8, -0.5);
    \draw[->] (zK) -- (1.6, -1.8);
                                           %                                            \draw[->] (z1) [in=180,out=0]  to node[right]{$\pi_1$} (g);
                                           %                                            \draw[->] (z2) [in=180,out=0]  to node[above]{$\pi_2$} (g);
                                           %                                            \draw[->,dashed] (zK) [in=180,out=0]  to node[right]{$\pi_K$} (g);
    \draw[->] (g) to (x);
    \begin{scope}[ thick, every node/.style={sloped,allow upside down}]
      \draw (2.1,0.9)-- (2.2, 0.);
      \draw (2.2,0.)-- node {\midarrow} (2.2,-2);
      \draw (2.2,-2)-- (m);
      \draw (m)-- (5, -2);
      \draw (5, -2)-- node {\midarrow} (5 ,-0.5);
      \draw (5, -0.5) -- (a);
      \draw (a)-- node {\midarrow} (2.2, -0.5);
    \end{scope}
    \node at (4.8, -1) {$s_{t}$};
    \node at (2.56, -0.25) {$s_{t+1}$};

  \end{tikzpicture}
  \caption{LatM-HMM Model defined by $\bm{H} = \{\Ss, \bm{q}, A, p({\bm{x}}|{\bm{s}}; \bm{\Phi}) \}$}
  \vspace{0.1cm}
\end{figure}

\textcolor{blue}{to be continued...}

Alternatively, we can use a latent-source mixed HMM (LatM-HMM) where different latent source share the same generator functioning as feature mapping. Then the generator of the LatM-HMM is defined as
\begin{equation}
  \left\{\bm{g}| \bm{g}: \bm{z}\rightarrow \bm{x}, s \in \Ss, \bm{z}\sim p_{s}(\bm{z})\right\}.
\end{equation}
We use $\bm{f} = \bm{g}^{-1}$ to denote inverse of $\bm{g}$ and use $\bm{\theta}$ to denote the parameter set of $\bm{g}$. Then the conditional probability for LatM-HMM is modeled as
\begin{align}
  p(\bm{x} | s; \bm{\Phi})
  &= p_s(\bm{z}) \bigg| \det \left( \pd{\bm{z}}{\bm{x}} \right) \bigg| \nonumber \\
  &= p_s(\bm{f}(\bm{x})) \bigg| \det \left( \pd{\bm{f}(\bm{x})}{\bm{x}} \right) \bigg|
\end{align}

The parameter set for this model to be decide is $\bm{\Phi} = \left\{ \bm{\theta},  \bm{\omega}_s, \forall s\in \Ss\right\}$.
Then the problem in \eqref{eq:sub-gm} can be reformulated as:
\begin{align}
  &\umax{\bm{\Phi}} \Qq(\bm{\Phi}; \bm{H}^{\mathrm{old}}) \nonumber \\
  =&\umax{ \bm{\theta}, \bm{\omega}_s, \forall s\in \Ss} \sum_{r=1}^{R} \sum_{t=1}^{{T}^{(r)}-1} \sum_{\smtr{s}{t}{r}=1}^{|\Ss|}  p(\smtr{s}{t}{r}| \ubmr{x}{r}; \bm{H}^{\mathrm{old}}) \left[\log\,p_{\smtr{s}{t}{r}}(\bm{f}(\bmtr{x}{t}{r})) + \log\,\bigg| \det\, \left( \pd{\bm{f}(\bmtr{x}{t}{r})}{\bmtr{x}{t}{r}} \right) \bigg| \right].
\end{align}







\section{Convergence Analysis}

\section{On Implementation of acoustic signal}
test citing \cite{2018arXiv180703039K}

For problem \eqref{eq:sub-gm} we are going to use our generative models to solve. I have the following consideration to revised our LatMM and GenMM for this application:
\begin{itemize}
\item Use factorized model instead of additive mixture model, to make likelihood computation logarithm domain compatible; 
\item Use full EM fashion instead of mini-batch fashion for training: store generative model as old for EM, there are always two neural networks working, one old for probability evaluation and one new for optimization.
\end{itemize}

\bibliography{myref}
\bibliographystyle{aaai}

     %      \bibliography{bibliography}


     %      \appendix

     %      \input{section/sec-appendixA}
\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
